{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGSaj/33Cb4cbiBepIkUO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guyfloki/ai-image-detector/blob/main/ai_image_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DOWNLOAD** **ZIP** **AND** **CSV**"
      ],
      "metadata": {
        "id": "fvle7WrWh_El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If it does not downloading due to high traffic on the file, please use directly link.\n",
        "#Train.zip\n",
        "!gdown --id 1-1ddgedsRSvJm3ERQwJPy4tq4cB0uWe9"
      ],
      "metadata": {
        "id": "XEYxNOL9gzsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test.zip\n",
        "!gdown --id 1-1xneYPH9fgSPCVnlZrhCM6c0FpFEl6B"
      ],
      "metadata": {
        "id": "_LjfRuOrg5ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train.csv\n",
        "!gdown --id 1rM2r7cxve7ApXCHTlBnMyD50n5hfnoAX"
      ],
      "metadata": {
        "id": "eaovvWU0g6H6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79d612a-3894-41b9-af92-6c30102e85d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rM2r7cxve7ApXCHTlBnMyD50n5hfnoAX\n",
            "To: /content/train.csv\n",
            "100% 100M/100M [00:02<00:00, 48.3MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test.csv\n",
        "!gdown --id 1-GzzsszBlrmUHaDvoqVJgFfLvzRQDaBV"
      ],
      "metadata": {
        "id": "dnMYru-_g62J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01dd54c-f138-4403-fc17-bdf860af35d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-GzzsszBlrmUHaDvoqVJgFfLvzRQDaBV\n",
            "To: /content/test.csv\n",
            "100% 10.7M/10.7M [00:00<00:00, 31.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNZIP** **FILES**"
      ],
      "metadata": {
        "id": "hLus1puriAQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Train.zip -d /content/"
      ],
      "metadata": {
        "id": "CHXyASREhXOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Test.zip -d /content/Test"
      ],
      "metadata": {
        "id": "FsPwSGFGhcGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECK** **SYSTEM** **DETAILS**"
      ],
      "metadata": {
        "id": "VLaAct_EiQV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_0ZUvzjAME0"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FxYgfm6AOkp"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN** **PROCESS**"
      ],
      "metadata": {
        "id": "uW1u9_vHiWS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ9ijMdHPRZ1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7tt0jGdQgN5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, AutoImageProcessor, EarlyStoppingCallback,AutoFeatureExtractor\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, log_loss\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoFeatureExtractor, CvtForImageClassification\n",
        "from safetensors.torch import load_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARE** **DATASET**"
      ],
      "metadata": {
        "id": "v04nraR3wMIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA2gKZbK0XAB"
      },
      "outputs": [],
      "source": [
        "# Data preparation\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.loc[idx, 'image_path']\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = int(self.data.loc[idx, 'target'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8EKJTwCah6W"
      },
      "outputs": [],
      "source": [
        "def compute_class_weights(n_samples_class0, n_samples_class1):\n",
        "    total = n_samples_class0 + n_samples_class1\n",
        "    weight_class0 = total / (2 * n_samples_class0)\n",
        "    weight_class1 = total / (2 * n_samples_class1)\n",
        "    return weight_class0, weight_class1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v80AZMZE0hwL"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y2E7R_n0_BT"
      },
      "outputs": [],
      "source": [
        "train_data = CustomDataset(\"/content/train.csv\", transform=transform)\n",
        "test_data = CustomDataset(\"/content/test.csv\", transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Q-UFtPa86a"
      },
      "outputs": [],
      "source": [
        "n_samples_class0 = sum(train_data.data[\"target\"] == 0.0)\n",
        "n_samples_class1 = sum(train_data.data[\"target\"] == 1.0)\n",
        "weight_class0, weight_class1 = compute_class_weights(n_samples_class0, n_samples_class1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqQR5Qo5a_Vp"
      },
      "outputs": [],
      "source": [
        "samples_weights = [weight_class0 if label == 0.0 else weight_class1 for label in train_data.data[\"target\"]]\n",
        "weighted_sampler = WeightedRandomSampler(samples_weights, len(samples_weights), replacement=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pBXEimbBFe"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=128, sampler=weighted_sampler, num_workers=6, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=6, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL** **DEFINITION**"
      ],
      "metadata": {
        "id": "JcvqkgoiwItY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkLCW2a1zGzI"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOMSeNOcbw01"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([weight_class0, weight_class1]).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dUwARBhh2Xd"
      },
      "outputs": [],
      "source": [
        "model = CvtForImageClassification.from_pretrained('microsoft/cvt-13')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOkUwHpnW_St"
      },
      "outputs": [],
      "source": [
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "        # First Hidden Layer\n",
        "        self.fc1 = nn.Linear(384, 256)\n",
        "        self.mish1 = nn.Mish(inplace=False)\n",
        "        self.norm1 = nn.BatchNorm1d(256)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Second Hidden Layer\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.mish2 = nn.Mish(inplace=False)\n",
        "        self.norm2 = nn.BatchNorm1d(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        # Output Layer\n",
        "        self.fc_out = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(self.norm1(self.mish1(self.fc1(x))))\n",
        "        x = self.dropout2(self.norm2(self.mish2(self.fc2(x))))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "model.classifier = CustomClassifier().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-5, eps=1e-8)"
      ],
      "metadata": {
        "id": "J6jimbK02uW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNiPizkXPioz"
      },
      "outputs": [],
      "source": [
        "scaler = GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QakiEOo-PKXz"
      },
      "source": [
        "**MAIN** **TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPZbEcEq27EI"
      },
      "outputs": [],
      "source": [
        "#change path for your own\n",
        "list_of_files = glob.glob('/content/drive/MyDrive/CVT-13_2/model_epoch_*.pth')\n",
        "if list_of_files:  # Check if the list is not empty\n",
        "    # Identify the latest model\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "\n",
        "    # Load the latest model\n",
        "    checkpoint = torch.load(latest_file)\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    starting_epoch = checkpoint['epoch'] + 1\n",
        "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = 1e-5\n",
        "    # Load the average loss\n",
        "    avg_loss_loaded = checkpoint.get('avg_loss', None)  # Use None if avg_loss is not found\n",
        "\n",
        "    model.train()  # or model.eval() if you are doing evaluation instead of training\n",
        "    total_epochs = 50\n",
        "else:\n",
        "    # If no saved model, start from epoch 1\n",
        "    total_epochs = 50\n",
        "    starting_epoch = 0\n",
        "    avg_loss_loaded = None\n",
        "    train_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph80_xEF27EQ"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for g in optimizer.param_groups:\n",
        "  print(g['lr'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckNuTzm97Ou-",
        "outputId": "6bdb3a1f-6040-4ed2-88d4-4a90cd7b0229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(starting_epoch, total_epochs):\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1}, Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}\", unit=\"batch\") as progress_bar:\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            # Gradient clipping\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "            progress_bar.update()\n",
        "\n",
        "            # Free up memory\n",
        "            del inputs, labels, outputs\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'avg_loss': avg_train_loss,\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        #change path for your own\n",
        "    }, f\"/content/drive/MyDrive/CVT-13_2/model_epoch_{epoch}.pth\")\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rNtGUiwx2tpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST**"
      ],
      "metadata": {
        "id": "VW0Qc8KC4zTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "# Assuming your training loop code ends before this\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():  # Disables gradient calculation for evaluation, which reduces memory usage\n",
        "    with tqdm(total=len(test_loader), desc=\"Evaluation\", unit=\"batch\") as progress_bar:\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():  # Use autocast if you're evaluating with mixed precision\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.logits, labels)  # Assuming your model outputs logits\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.logits.max(1)  # Get the index of the max log-probability\n",
        "            all_predicted.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            progress_bar.set_postfix({\"Test Loss\": loss.item()})\n",
        "            progress_bar.update()\n",
        "\n",
        "# Convert all_labels and all_predicted to numpy arrays if they are not already\n",
        "all_labels = np.array(all_labels)\n",
        "all_predicted = np.array(all_predicted)\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "accuracy = accuracy_score(all_labels, all_predicted)\n",
        "precision = precision_score(all_labels, all_predicted, average='macro')\n",
        "recall = recall_score(all_labels, all_predicted, average='macro')\n",
        "f1 = f1_score(all_labels, all_predicted, average='macro')\n",
        "\n",
        "print(f'Average Test Loss: {avg_test_loss:.4f}')\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se6sXb242trq",
        "outputId": "a476b913-08c1-4905-dc8a-4eacdd6df353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: 100%|██████████| 1978/1978 [07:54<00:00,  4.17batch/s, Test Loss=1.33e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Loss: 0.1275\n",
            "Accuracy: 98.54%\n",
            "Precision: 0.99\n",
            "Recall: 0.98\n",
            "F1 Score: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=False)  # You can set show_normed to True to show percentages\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "plt.savefig(\"confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "SF4JH8VctNqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_losses is a list that contains the average loss of each epoch\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"loss.png\")"
      ],
      "metadata": {
        "id": "WYYA8dQ8gDET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAKE** **PREDICTION**"
      ],
      "metadata": {
        "id": "-EWEaDbTvxtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "6to2zSQJFjx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to load an image from a URL\n",
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    return img\n",
        "\n",
        "# URL of your custom image\n",
        "image_url = \"\"\n",
        "\n",
        "# Load and transform your custom image\n",
        "custom_image = load_image_from_url(image_url)\n",
        "transformed_image = transform(custom_image).unsqueeze(0)  # Add batch dimension\n",
        "transformed_image = transformed_image.to(device)\n",
        "\n",
        "# Evaluate the custom image using the model\n",
        "model.eval()\n",
        "with torch.no_grad():  # Disables gradient calculation for evaluation\n",
        "    # If you're using autocast for mixed precision\n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model(transformed_image)\n",
        "        # Use the logits attribute to get the prediction scores\n",
        "        logits = outputs.logits\n",
        "        _, predicted = logits.max(1)  # Get the index of the max log-probability\n",
        "\n",
        "# Print the prediction\n",
        "print(f'Predicted class: {predicted.item()}')\n",
        "\n",
        "# If you want to get the probabilities\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "print(f'Class probabilities: {probabilities}')\n"
      ],
      "metadata": {
        "id": "gbMF1rge2tuA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}